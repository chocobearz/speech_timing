{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptut0\\Documents\\speech_timing\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "from transformers import BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\ptut0\\\\Documents\\\\speech_timing\\\\basline\\\\data\\\\clean_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptDirectory = r\"C:\\\\Users\\\\ptut0\\\\Documents\\\\speech_timing\\\\basline\\\\data\\\\txt\\\\clean\\\\\"\n",
    "writeDirectory = \"C:\\\\Users\\\\ptut0\\\\Documents\\\\speech_timing\\\\basline\\\\data\\\\\"\n",
    "ssmlTags = ['x-slow', 'slow', 'medium', 'fast', 'x-fast', 'x-weak', 'weak', 'medium-p', 'strong', 'x-strong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {'x-slow': 1, 'slow' : 2, 'medium' : 3, 'fast' : 4, 'x-fast': 5,  'x-weak' : 6, 'weak' : 7, 'medium-p' : 8, 'strong' : 9, 'x-strong' : 10}\n",
    "ids_to_labels = {1: 'x-slow', 2: 'slow', 3 : 'medium', 4 : 'fast', 5: 'x-fast', 6 : 'x-weak', 7 : 'weak', 8 : 'medium-p', 9 : 'strong', 10 :  'x-strong'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label(scripts, labels, pause_labels):\n",
    "  tokenized_inputs = tokenizer(scripts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "  pausePunct = ',!?.'\n",
    "  pauseTracker = 0\n",
    "\n",
    "  word_ids = tokenized_inputs.word_ids()\n",
    "  input_ids = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "  #print(f\"labels: {len(labels)}\")\n",
    "  #print(f\"pause labels: {len(pause_labels)}\")\n",
    "  #print(f\"script: {len(scripts.split())}\")\n",
    "  #print(f\" punct : {scripts.count('.') + scripts.count('!') + scripts.count('?') + scripts.count(',')}\")\n",
    "\n",
    "  previous_word_idx = None\n",
    "  label_ids = []\n",
    "\n",
    "  # to move forward at punctuation\n",
    "  m = 0\n",
    "\n",
    "  for i, word_idx in enumerate(word_ids):\n",
    "    if word_idx is None:\n",
    "      label_ids.append(-100)\n",
    "    # at punctuation\n",
    "    elif input_ids[i] in string.punctuation:\n",
    "      # for a pause add a pause tag\n",
    "      if input_ids[i] in pausePunct:\n",
    "        if word_ids[i + 1] != None:\n",
    "          label_ids.append(labels_to_ids[pause_labels[pauseTracker]])\n",
    "          pauseTracker += 1\n",
    "          m += 1\n",
    "        # end of sentence has no pause\n",
    "        else:\n",
    "          label_ids.append(-100)\n",
    "          m += 1\n",
    "      # other punctuation no tag\n",
    "      else:\n",
    "        label_ids.append(-100)\n",
    "        m += 1\n",
    "    # combine words with apostrophe to one word\n",
    "    elif input_ids[i-1] == \"'\":\n",
    "      label_ids.append(-100)\n",
    "      m +=1\n",
    "    # new word\n",
    "    elif word_idx != previous_word_idx:\n",
    "      try:\n",
    "        label_ids.append(labels_to_ids[labels[word_idx-m]])\n",
    "      except:\n",
    "        label_ids.append(-100)\n",
    "    # word with multiple tags\n",
    "    else:\n",
    "      label_ids.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "  return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "\n",
    "    lb = [ast.literal_eval(i) for i in data['discrete_lengths'].values.tolist()]\n",
    "    plb = [ast.literal_eval(i) for i in df['discrete_pauses'].values.tolist()]\n",
    "    txt = df['script'].values.tolist()\n",
    "    #file = df['filename'].values.tolist()\n",
    "    self.texts = [tokenizer(\n",
    "        str(i),\n",
    "        padding='max_length',\n",
    "        max_length = 512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "      ) for i in txt\n",
    "    ]\n",
    "    self.labels = [align_label(i,j,k) for i,j,k in zip(txt, lb, plb)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def get_batch_data(self, idx):\n",
    "    return self.texts[idx]\n",
    "\n",
    "  def get_batch_labels(self, idx):\n",
    "    return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_data = self.get_batch_data(idx)\n",
    "    batch_labels = self.get_batch_labels(idx)\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BertModel, self).__init__()\n",
    "    self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(ssmlTags))\n",
    "\n",
    "  def forward(self, input_id, mask, label):\n",
    "    output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = np.split(data.sample(frac=1, random_state=69),\n",
    "                            [int(.8 * len(data)), int(.9 * len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train_loop(model, df_train, df_val):\n",
    "  train_dataset = DataSequence(df_train)\n",
    "  val_dataset = DataSequence(df_val)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  # can set momentum\n",
    "  optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  best_acc = 0\n",
    "  best_loss = 1000\n",
    "\n",
    "  for epoch_num in range(EPOCHS):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    model.train()\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "      for i in range(logits.shape[0]):\n",
    "        # get rid of any with the hold label\n",
    "        logits_clean = logits[i][train_label[i] != -100]\n",
    "        label_clean = train_label[i][train_label[i] != -100]\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_train += acc\n",
    "        total_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "          # get rid of any with the hold label\n",
    "          val_label = val_label.to(device)\n",
    "          mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "          input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "          loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "          for i in range(logits.shape[0]):\n",
    "            logits_clean = logits[i][val_label[i] != -100]\n",
    "            label_clean = val_label[i][val_label[i] != -100]\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226\n",
      "20\n",
      "20\n",
      "1266\n",
      "1266\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "from transformers import BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "#disable parellelism warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\ptut0\\\\Documents\\\\speech_timing\\\\basline\\\\data\\\\clean_baseline.csv\")\n",
    "\n",
    "#ssmlTags = set({'x-slow', 'slow', 'medium', 'fast', 'x-fast', 'x-weak', 'weak', 'medium-p', 'strong', 'x-strong'})\n",
    "ssmlTags = set({'x-slow', 'slow', 'medium', 'fast', 'x-fast'})\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "#labels_to_ids = {'x-slow': 0, 'slow' : 1, 'medium' : 2, 'fast' : 3, 'x-fast': 4,  'x-weak' : 5, 'weak' : 6, 'medium-p' : 7, 'strong' : 8, 'x-strong' : 9}\n",
    "#ids_to_labels = {0: 'x-slow', 1: 'slow', 2 : 'medium', 3 : 'fast', 4: 'x-fast', 5 : 'x-weak', 6 : 'weak', 7 : 'medium-p', 8 : 'strong', 9 : 'x-strong'}\n",
    "labels_to_ids = {'x-slow': 0, 'slow' : 1, 'medium' : 2, 'fast' : 3, 'x-fast': 4}\n",
    "ids_to_labels = {0: 'x-slow', 1: 'slow', 2 : 'medium', 3 : 'fast', 4: 'x-fast'}\n",
    "\n",
    "def align_label(scripts, labels, pause_labels):\n",
    "  tokenized_inputs = tokenizer(scripts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "  pausePunct = ',!?.'\n",
    "  pauseTracker = 0\n",
    "\n",
    "  word_ids = tokenized_inputs.word_ids()\n",
    "  input_ids = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "  #print(f\"labels: {len(labels)}\")\n",
    "  #print(f\"pause labels: {len(pause_labels)}\")\n",
    "  #print(f\"script: {len(scripts.split())}\")\n",
    "  #print(f\" punct : {scripts.count('.') + scripts.count('!') + scripts.count('?') + scripts.count(',')}\")\n",
    "\n",
    "  previous_word_idx = None\n",
    "  label_ids = []\n",
    "\n",
    "  # to move forward at punctuation\n",
    "  m = 0\n",
    "\n",
    "  for i, word_idx in enumerate(word_ids):\n",
    "    if word_idx is None:\n",
    "      label_ids.append(-100)\n",
    "    # at punctuation\n",
    "    elif input_ids[i] in string.punctuation:\n",
    "      label_ids.append(-100)\n",
    "      m += 1\n",
    "      # for a pause add a pause tag\n",
    "      #if input_ids[i] in pausePunct:\n",
    "      #  if word_ids[i + 1] != None:\n",
    "      #    label_ids.append(labels_to_ids[pause_labels[pauseTracker]])\n",
    "      #    pauseTracker += 1\n",
    "      #    m += 1\n",
    "      #  # end of sentence has no pause\n",
    "      #  else:\n",
    "      #    label_ids.append(-100)\n",
    "      #    m += 1\n",
    "      ## other punctuation no tag\n",
    "      #else:\n",
    "      #  label_ids.append(-100)\n",
    "      #  m += 1\n",
    "    # combine words with apostrophe to one word\n",
    "    elif input_ids[i-1] == \"'\":\n",
    "      label_ids.append(-100)\n",
    "      m +=1\n",
    "    # new word\n",
    "    elif word_idx != previous_word_idx:\n",
    "      try:\n",
    "        label_ids.append(labels_to_ids[labels[word_idx-m]])\n",
    "      except:\n",
    "        label_ids.append(-100)\n",
    "    # word with multiple tags\n",
    "    else:\n",
    "      label_ids.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "  return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "\n",
    "    lb = [ast.literal_eval(i) for i in data['discrete_lengths'].values.tolist()]\n",
    "    plb = [ast.literal_eval(i) for i in df['discrete_pauses'].values.tolist()]\n",
    "    txt = df['script'].values.tolist()\n",
    "    #file = df['filename'].values.tolist()\n",
    "    self.texts = [tokenizer(\n",
    "        str(i),\n",
    "        padding='max_length',\n",
    "        max_length = 512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "      ) for i in txt\n",
    "    ]\n",
    "    self.labels = [align_label(i,j,k) for i,j,k in zip(txt, lb, plb)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def get_batch_data(self, idx):\n",
    "    return self.texts[idx]\n",
    "\n",
    "  def get_batch_labels(self, idx):\n",
    "    return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_data = self.get_batch_data(idx)\n",
    "    batch_labels = self.get_batch_labels(idx)\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BertModel, self).__init__()\n",
    "    self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(ssmlTags))\n",
    "\n",
    "  def forward(self, input_id, mask, label):\n",
    "    output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "    return output\n",
    "\n",
    "def train_loop(model, df_train, df_val):\n",
    "  train_dataset = DataSequence(df_train)\n",
    "  val_dataset = DataSequence(df_val)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  # can set momentum\n",
    "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  val_loss = []\n",
    "  val_accuracy = []\n",
    "  train_loss = []\n",
    "  train_accuracy = []\n",
    "  for epoch_num in range(EPOCHS):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    model.train()\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "      for i in range(logits.shape[0]):\n",
    "        # get rid of any with the hold label\n",
    "        logits_clean = logits[i][train_label[i] != -100]\n",
    "        label_clean = train_label[i][train_label[i] != -100]\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_train += acc\n",
    "        total_loss_train += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    #train_loss.append(total_loss_train/len(df_train))\n",
    "    #train_accuracy.append(total_acc_train/len(df_train))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      total_acc_val = 0\n",
    "      total_loss_val = 0\n",
    "\n",
    "      for val_data, val_label in val_dataloader:\n",
    "        # get rid of any with the hold label\n",
    "        val_label = val_label.to(device)\n",
    "        mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "        input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "        loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "        for i in range(logits.shape[0]):\n",
    "          logits_clean = logits[i][val_label[i] != -100]\n",
    "          label_clean = val_label[i][val_label[i] != -100]\n",
    "          predictions = logits_clean.argmax(dim=1)\n",
    "          acc = (predictions == label_clean).float().mean()\n",
    "          total_acc_val += acc\n",
    "          total_loss_val += loss.item()\n",
    "    #val_loss.append(total_loss_val/len(df_train))\n",
    "    #val_accuracy.append(total_acc_val/len(df_train))\n",
    "\n",
    "    print(f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "\n",
    "def evaluate(model, df_test):\n",
    "\n",
    "    test_dataset = DataSequence(df_test)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "            input_id = test_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, test_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][test_label[i] != -100]\n",
    "              label_clean = test_label[i][test_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_test += acc\n",
    "\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'].to(device)\n",
    "    input_id = text['input_ids'].to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "\n",
    "# not done\n",
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "df_train, df_val, df_test = np.split(data.sample(frac=1, random_state=69),\n",
    "                            [1226, 1246]) \n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_val))  \n",
    "print(len(data))\n",
    "print(len(df_train)+len(df_test)+len(df_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "050f688ab4d1c2fe858c337d83d81f019d7845133528d51eea4c92bf3c13162b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
